{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Web_Scrapper.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXVxg6A43For",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import ssl\n",
        "import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmE6yHs-3K8C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#header for metadata\n",
        "headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hfha9xpf5RIb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get all the urls to scrape from search url\n",
        "def get_all_anchor_tags(url):\n",
        "  page = requests.get(url, headers=headers)#, proxies=proxies)\n",
        "  content = page.content\n",
        "  soup = BeautifulSoup(content)\n",
        "  link_list = []\n",
        "  for tag in soup.find_all('a'):\n",
        "    link = tag.get(\"href\")\n",
        "    searchd = \"/in/buy/projects/page/\"\n",
        "    if link.find(searchd) != -1:\n",
        "      new_link = \"https://housing.com\" + link\n",
        "      link_list.append(new_link)\n",
        "  return link_list\n",
        "\n",
        "#Scrape the url\n",
        "def extract_data(url):\n",
        "  page = requests.get(url, headers=headers)#, proxies=proxies)\n",
        "  content = page.content\n",
        "  soup = BeautifulSoup(content)\n",
        "  #print(soup)\n",
        "  result = {}\n",
        "  flat_data = soup.findAll('div',attrs={'class' : 'css-j2zgcq'})\n",
        "  flat_type = flat_data[0].find('h1',attrs={'class':'css-10rvbm3'})\n",
        "  if flat_type is not None:\n",
        "    flat_type = flat_type.text\n",
        "  else:\n",
        "    flat_type = \"Flat type not mentioned\"\n",
        "  result['Flat type'] = flat_type\n",
        "  key = []\n",
        "  value = []\n",
        "  for data in soup.findAll('section',attrs={'class' : 'css-13dph6'}):\n",
        "    for i in data.findAll('div',attrs={'class' : 'css-r74jsk'}):\n",
        "      key.append(i.text)\n",
        "    for j in data.findAll('div',attrs={'class' : 'css-3o6ku8'}):\n",
        "      value.append(j.text)\n",
        "  for i in range(0,4):\n",
        "    result[key[i]] = value[i]\n",
        "  #print(result)\n",
        "  tables = soup.find_all(\"table\")\n",
        "  table = tables[0]\n",
        "  tab_data = [[cell.text for cell in row.find_all([\"th\",\"td\"])]\n",
        "                          for row in table.find_all(\"tr\")]\n",
        "  for k in tab_data:\n",
        "    result[k[0]] = k[1]\n",
        "  #print(result)\n",
        "\n",
        "  about = soup.find('div',attrs={'class' : 'css-1tetu0c'})\n",
        "  #print(about.text)\n",
        "  about_data = soup.find('div',attrs={'class' : 'about-text css-1d1e0rh'})\n",
        "  #print(about_data.text)\n",
        "  result[about.text] = about_data.text\n",
        "\n",
        "  special_high = soup.find('div',attrs={'class' : \"css-1o20zr1\"})\n",
        "\n",
        "  if special_high is not None:\n",
        "    high = special_high.find('span',attrs={'class' : \"css-xms1sc\"}).text\n",
        "    lis = []\n",
        "    for i in special_high.findAll('div',attrs={'class' : \"highlight css-1byt3mr\"}):\n",
        "      lis.append(i.text)\n",
        "    result[high] = \",\".join(lis)\n",
        "  #print(result)\n",
        "  new_dic = {}\n",
        "  for k, v in result.items():\n",
        "    lis=[]\n",
        "    if v.find(\"}\") != -1:\n",
        "      lis.append(v.split(\"}\")[-1])\n",
        "    elif v.find(\"₹\") != -1:\n",
        "      lis.append(v.split(\"₹\")[-1])\n",
        "    else:\n",
        "      lis.append(v)\n",
        "    \n",
        "    new_dic[k] = lis\n",
        "  df = pd.DataFrame(new_dic)\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-jowDLxkVc8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_image_urls(url):\n",
        "  page = requests.get(url, headers=headers)#, proxies=proxies)\n",
        "  content = page.content\n",
        "  soup = BeautifulSoup(content)\n",
        "  images = soup.find_all('img')\n",
        "  images_list = []\n",
        "  for image in images: \n",
        "    images_list.append(image['src'])\n",
        "  return images_list\n",
        "def download_images(img_list):\n",
        "  count = 0\n",
        "  for image in img_list:\n",
        "    try:\n",
        "      response = requests.get(image, stream=True)\n",
        "      #print(response,image)\n",
        "      img_format = image.split(\".\")[-1]\n",
        "      remaining = image.split(\".\")[-2]\n",
        "      realname = remaining.split(\"/\")[-1]\n",
        "      realname = realname + \"_\" + str(count)+\".\" + img_format\n",
        "      # progress bar, changing the unit to bytes instead of iteration (default by tqdm)\n",
        "      count+=1\n",
        "      with open(realname, \"wb\") as f:\n",
        "          f.write(response.content)\n",
        "    except:\n",
        "      continue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj0s6usUaVoC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Write the data to the csv file\n",
        "def write_to_file(dataframe):\n",
        "  dataframe.to_csv('my_csv.csv', mode='w',header=True,index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJLYfssX6O8s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#merge the new and existing data in the csv file\n",
        "def process_data_frame(df):\n",
        "  try:\n",
        "    old_df = pd.read_csv(\"my_csv.csv\")\n",
        "    new_df = old_df.append(df)\n",
        "  except:\n",
        "    new_df = df\n",
        "  return new_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KljiJtoakBzb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#urls of the search page\n",
        "#TODO\n",
        "# Add your URLS to search in the url list\n",
        "# Note : Don't forget to add a comma after each url\n",
        "url_list = [\n",
        "          \"https://housing.com/in/buy/searches/P3te34s1zvaong5td\",\n",
        "          \"https://housing.com/in/buy/searches/P2q1uwqz5uo79lvmy\",\n",
        "          \"https://housing.com/in/buy/searches/P6w6rzu8yq0jutxih\",\n",
        "          \"https://housing.com/in/buy/searches/P1el4pqs71cc8a4oq\",\n",
        "          \"https://housing.com/in/buy/searches/Presjxke82ehhai1\",\n",
        "          \"https://housing.com/in/buy/searches/P2p755bijb3w8veen\",\n",
        "          \"https://housing.com/in/buy/searches/P6mmiqcl6ho6er3oc\",\n",
        "          \"https://housing.com/in/buy/searches/E4ff9\",\n",
        "          \"https://housing.com/in/buy/searches/Pv26iopup2rxged3\",\n",
        "          \"https://housing.com/in/buy/searches/E4e7y\",\n",
        "          \"https://housing.com/in/buy/searches/E4di3\",\n",
        "          \"https://housing.com/in/buy/searches/P2hln47z1q71v8uqk\"\n",
        "          \"https://housing.com/in/buy/searches/P1jlgdrtnm1iwir8c\",\n",
        "          \"https://housing.com/in/buy/searches/P26nrckh1cj7f6fhh\"\n",
        "\n",
        "]\n",
        "for url in url_list:\n",
        "  # Ignore SSL certificate errors\n",
        "  ctx = ssl.create_default_context()\n",
        "  ctx.check_hostname = False\n",
        "  ctx.verify_mode = ssl.CERT_NONE\n",
        "  #get all the url from search result\n",
        "  list_of_urls = get_all_anchor_tags(url)\n",
        "  print(list_of_urls)\n",
        "  for i in list_of_urls:\n",
        "    try:\n",
        "      df = extract_data(i)\n",
        "      new_df = process_data_frame(df)\n",
        "      write_to_file(new_df)\n",
        "      #code to download images from the website\n",
        "      #====\n",
        "      # list_of_img_urls = extract_image_urls(i)\n",
        "      # download_images(list_of_img_urls)\n",
        "      #=====\n",
        "    except:\n",
        "      continue\n",
        "#print the data\n",
        "read_data = pd.read_csv(\"my_csv.csv\");\n",
        "read_data.head()  \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}